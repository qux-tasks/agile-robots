GENERAL
- All development was done on Ubuntu, so there will likely be some discrepancies in behavior and 
    environment setup on other systems.
- I made the decision to create two separate frameworks because we are testing different platforms. 
    If it were a single platform, I would have simply organized the tests into different suites within
     one framework.
- Both frameworks include reporting functionality. After test execution, if they were run with the 
    required flag, a reports directory will appear in the project root, containing the generated report.
- It is highly recommended to set up the environment using the corresponding README.md files located 
    in the root directories of the projects.

============================================================

TASK 1
In this task, there were a vast number of errors in the API's behavior. Therefore, I proceeded as follows:

PASS tests - These are the tests that work correctly, and I am confident about them.
FAIL tests - These tests fail due to defects in the API's operation.
SKIPPED tests - For the most part, these also fail, largely due to API defects. However, I am uncertain about the 
specific expected status code for these tests. The documentation omits this information, and in all cases, it's a 
debatable point, so these codes need to be clarified.

Furthermore, I intentionally did not write tests for various token-related scenarios because I don't know what the 
expected result should be. Examples of such unwritten tests include: working with expired tokens (the functionality 
is not described), stress testing (e.g., sending a huge number of requests from a single token), a large volume of 
token requests (how the server should behave), and so on.

CONCLUSION: The API partially meet business expectations (but it should be clarified, because I didn't see full r
equirements list and don't know all details) but has a lot of defects. It works incorrectly, not securely. 
It's okay for development process, but it's not okay for some release =)

============================================================

TASK 2
The testing was performed using the python/playwright/pytest stack.
Currently, two test cases are failing, clearly due to defects. These are common scenarios: leading/trailing 
spaces in the login are not handled, and the login is not case-sensitive.
What I would like to clarify: should the logo contain a link to the home page? Currently, there is no link, 
which is why such a check is not implemented.
I also encountered an issue where during the initial launches (after clearing the cache), the website would 
switch to a different language, even though my system language is set to English. Because of this, approximately 
10 tests might fail, as they verify the correctness of various labels and texts based specifically on the 
English language. In one instance, the language changed to Spanish (or Portuguese), and in another, to Chinese. 
This is also something to keep in mind.

CONCLUSION: The login form works, but has at least 2 critical defects (2 failed test cases). Also, there is one 
defect about page language changing. I will not recommend it for release =)